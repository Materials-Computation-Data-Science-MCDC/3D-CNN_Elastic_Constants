{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08abcd-4845-4497-8702-7f1596e3931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a54ea-a6c9-4e73-bdf7-96ccf37d2013",
   "metadata": {},
   "source": [
    "The code below has been used to train only on binaries dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941bf08-6b58-40f2-9f05-2a03bf81cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_indices(total_binary_count, total_data_count, train_ratio, seed=42): ## different seed tested such as 42 , 123 , 72  #27, 45, 99\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    binary_indices = list(range(total_binary_count))\n",
    "    train_count = int(total_binary_count * train_ratio)\n",
    "    train_indices = random.sample(binary_indices, train_count)\n",
    "    all_indices = list(range(total_data_count))\n",
    "    test_indices = [i for i in all_indices if i not in train_indices]\n",
    "    return train_indices, test_indices\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mse = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    sum_squared_true = tf.reduce_sum(tf.square(y_true))\n",
    "    epsilon = 1e-7\n",
    "    sum_squared_true = sum_squared_true + epsilon\n",
    "    loss = mse / sum_squared_true\n",
    "    return loss\n",
    "\n",
    "def build_model(kernel_size, dropout):\n",
    "    conv1 = layers.Conv3D(16, kernel_size, activation='relu', data_format=\"channels_last\", padding=\"same\")\n",
    "    ap1 = layers.MaxPooling3D(pool_size=(2, 2, 2))\n",
    "    conv2 = layers.Conv3D(32, kernel_size, activation='relu', data_format=\"channels_last\", padding=\"same\")\n",
    "    ap2 = layers.MaxPooling3D(pool_size=(2, 2, 2))\n",
    "    conv3 = layers.Conv3D(64, kernel_size, activation='relu', data_format=\"channels_last\", padding=\"same\")\n",
    "    ap3 = layers.MaxPooling3D(pool_size=(2, 2, 2))\n",
    "    flat = layers.Flatten()\n",
    "    drop1 = layers.Dropout(rate=dropout)\n",
    "    dense1 = layers.Dense(2048, activation='relu')\n",
    "    drop2 = layers.Dropout(rate=dropout)\n",
    "    dense2 = layers.Dense(1024, activation='relu')\n",
    "    dense3 = layers.Dense(8, name='parameters')  \n",
    "    input_layer = keras.Input(shape=(56, 40, 40, 1))\n",
    "    x = conv1(input_layer)\n",
    "    x = ap1(x)\n",
    "    x = conv2(x)\n",
    "    x = ap2(x)\n",
    "    x = conv3(x)  \n",
    "    x = ap3(x)\n",
    "    x = flat(x)\n",
    "    x = dense1(x)\n",
    "    x = drop1(x)\n",
    "    x = dense2(x)\n",
    "    x = drop2(x)\n",
    "    outputs = dense3(x)\n",
    "    model = keras.Model(inputs=input_layer, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af762913-14cc-4943-a4d2-ba2c2c7e1fd7",
   "metadata": {},
   "source": [
    "The code below has been used to train on entire binaries dataset plus terneries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8071f74d-5f23-4af1-a59d-72203c9b9713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_indices(total_binary_count, total_data_count, train_ratio, seed=42): #different seed tested such as 1234, 42, 32\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    binary_indices = list(range(total_binary_count))\n",
    "    train_indices = binary_indices[:1110]\n",
    "    remaining_indices_count = total_data_count - len(train_indices)\n",
    "    additional_train_count = int((remaining_indices_count - 78) * train_ratio)  \n",
    "    additional_train_indices = random.sample(range(1110, 1473), additional_train_count)\n",
    "    train_indices.extend(additional_train_indices)\n",
    "    all_indices = list(range(total_data_count))\n",
    "    test_indices = [i for i in all_indices if i not in train_indices]\n",
    "    return train_indices, test_indices\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mse = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    sum_squared_true = tf.reduce_sum(tf.square(y_true))\n",
    "    epsilon = 1e-7\n",
    "    sum_squared_true = sum_squared_true + epsilon\n",
    "    loss = mse / sum_squared_true\n",
    "    return loss\n",
    "\n",
    "def build_model(kernel_size, dropout):\n",
    "    # Define layers\n",
    "    conv1 = layers.Conv3D(16, kernel_size, activation='relu', data_format=\"channels_last\", padding=\"same\")\n",
    "    ap1 = layers.MaxPooling3D(pool_size=(2, 2, 2))\n",
    "    conv2 = layers.Conv3D(32, kernel_size, activation='relu', data_format=\"channels_last\", padding=\"same\")\n",
    "    ap2 = layers.MaxPooling3D(pool_size=(2, 2, 2))\n",
    "    conv3 = layers.Conv3D(64, kernel_size, activation='relu', data_format=\"channels_last\", padding=\"same\")\n",
    "    ap3 = layers.MaxPooling3D(pool_size=(2, 2, 2))\n",
    "    flat = layers.Flatten()\n",
    "    drop1 = layers.Dropout(rate=dropout)\n",
    "    dense1 = layers.Dense(2048, activation='relu')\n",
    "    drop2 = layers.Dropout(rate=dropout)\n",
    "    dense2 = layers.Dense(1024, activation='relu')\n",
    "    dense3 = layers.Dense(8, name='parameters')  \n",
    "    input_layer = keras.Input(shape=(56, 40, 40, 1))\n",
    "    x = conv1(input_layer)\n",
    "    x = ap1(x)\n",
    "    x = conv2(x)\n",
    "    x = ap2(x)\n",
    "    x = conv3(x)  \n",
    "    x = ap3(x)\n",
    "    x = flat(x)\n",
    "    x = dense1(x)\n",
    "    x = drop1(x)\n",
    "    x = dense2(x)\n",
    "    x = drop2(x)\n",
    "    outputs = dense3(x)\n",
    "    model = keras.Model(inputs=input_layer, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244379c-b9cb-4bfe-bf31-9dcea202e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### These are input data including the images and corresponding properties\n",
    "images = np.load(\"merged_array.npy\")   \n",
    "properties = pd.read_csv(\"merged_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701f97f4-832c-4778-b6f7-52430f7f3773",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = generate_train_test_indices(total_binary_count=1110, total_data_count=1550, train_ratio=0.25)  ## train_ratio might change based on the number of images use for training process\n",
    "X_train = images[train_indices]\n",
    "X_test = images[test_indices]\n",
    "y_train= properties.iloc[train_indices]\n",
    "y_test = properties.iloc[test_indices]\n",
    "## To select only some specific properties we need to adjust the code below. This code consider all 8 elastic properties\n",
    "columns_to_normalize = ['c11', 'c12', 'c44', 'G', 'B', 'E_VRH', 'nu', 'Cohesive_energy']\n",
    "scaler = MinMaxScaler()\n",
    "y_train[columns_to_normalize] = scaler.fit_transform(y_train[columns_to_normalize])\n",
    "y_train = y_train[['c11', 'c12', 'c44', 'G', 'B', 'E_VRH', 'nu', 'Cohesive_energy']]\n",
    "y_test[columns_to_normalize] = scaler.fit_transform(y_test[columns_to_normalize])\n",
    "y_test = y_test[['c11', 'c12', 'c44', 'G', 'B', 'E_VRH', 'nu', 'Cohesive_energy']]\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "np.random.seed(42)\n",
    "val_split = 0.1\n",
    "val_size = int(X_train.shape[0] * val_split)\n",
    "val_indices = np.random.choice(X_train.shape[0], val_size, replace=False)\n",
    "val_data = (X_train[val_indices], y_train.iloc[val_indices])\n",
    "train_data = (np.delete(X_train, val_indices, axis=0), y_train.drop(index=val_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942fb96-cb45-44d0-835e-0445624bb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = [i for i in train_indices if i not in val_indices]\n",
    "all_indices = np.arange(0, 1550)\n",
    "data = []\n",
    "for idx in all_indices:\n",
    "    if idx in val_indices:\n",
    "        set_type = 'validation'\n",
    "    elif idx in train_indices:\n",
    "        set_type = 'training'\n",
    "    elif idx in test_indices:\n",
    "        set_type = 'test'\n",
    "    else:\n",
    "        set_type = 'none'\n",
    "    data.append((idx, set_type))\n",
    "df_25B = pd.DataFrame(data, columns=['index', 'set_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb559f-c635-442c-8492-fc5c02124f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model((3, 3, 11), 0.3)\n",
    "checkpoint_callback = ModelCheckpoint(\"May14_allproperties_25B.h5\", monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
    "reduce_lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=30, min_lr=1e-7)\n",
    "model.compile(optimizer='adam', loss=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d4e94-ef36-4436-b240-461568952bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data[0], train_data[1], \n",
    "                    batch_size=128, \n",
    "                    epochs=5000, \n",
    "                    validation_data=val_data,\n",
    "                    callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48958978-6470-4b7d-82ac-7802be71caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv('May14_allproperties_25B.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb991e-9c2b-4f0f-b21c-5669197a376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"May14_allproperties_25B.h5\")\n",
    "predictions = model.predict(images)\n",
    "denormalized_predictions = scaler.inverse_transform(predictions)\n",
    "columns_to_normalize = ['c11', 'c12', 'c44', 'G', 'B', 'E_VRH', 'nu', 'Cohesive_energy']\n",
    "denormalized_df = pd.DataFrame(denormalized_predictions, columns=columns_to_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46bd8c8-fd41-4c71-b91f-7634d15fbf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"May14_allproperties_100B_25T.h5\")\n",
    "predictions = model.predict(images)\n",
    "denormalized_predictions = scaler.inverse_transform(predictions)\n",
    "columns_to_normalize = ['c11', 'c12', 'c44', 'G', 'B', 'E_VRH', 'nu', 'Cohesive_energy']\n",
    "denormalized_df = pd.DataFrame(denormalized_predictions, columns=columns_to_normalize)\n",
    "assert denormalized_df.shape[0] == df_100B_25T.shape[0], \"The number of rows in both DataFrames must be the same.\"\n",
    "merged_df = pd.concat([df_100B_25T, denormalized_df], axis=1)\n",
    "merged_df.to_excel('results_100B_25T.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3154dfa-022c-427f-a0ce-844ec74e8251",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"May14_allproperties_25B.h5\")\n",
    "predictions = model.predict(X_test)\n",
    "r2_scores = []\n",
    "for i in range(predictions.shape[1]):\n",
    "    r2 = r2_score(y_test.iloc[:, i], predictions[:, i])\n",
    "    r2_scores.append(r2)\n",
    "print(\"R2-scores for each target parameter:\", r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd6d02-6212-4347-98ac-c8f3eafe8070",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(train_data[0])\n",
    "r2_scores = []\n",
    "for i in range(predictions.shape[1]):\n",
    "    r2 = r2_score(train_data[1].iloc[:, i], predictions[:, i])\n",
    "    r2_scores.append(r2)\n",
    "print(\"R2-scores for each target parameter:\", r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed3203-dbd5-4147-bdcf-406c308f33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_data[0])\n",
    "r2_scores = []\n",
    "for i in range(predictions.shape[1]):\n",
    "    r2 = r2_score(val_data[1].iloc[:, i], predictions[:, i])\n",
    "    r2_scores.append(r2)\n",
    "print(\"R2-scores for each target parameter:\", r2_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
